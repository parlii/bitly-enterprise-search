{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/Users/drizal/parli/keys/bitly-ai-experiments-f0eacba17094.json\"\n",
    "\n",
    "BUCKET_NAME = \"bitly-enterprise-search-docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade google-cloud-storage\n",
    "%pip install google-cloud-aiplatform\n",
    "%pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.language_models import TextGenerationModel\n",
    "model = TextGenerationModel.from_pretrained('text-bison@latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "prompt = f\"\"\"Tell me a joke:  \"\"\"\n",
    "result = model.predict(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a directory called prepared_data\n",
    "!mkdir prepared_data\n",
    "!mkdir prepared_data/clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# walk through current directory and subsirectory\n",
    "# get all md files and clean up any html code it may have\n",
    "# save the cleaned up text to a file in the prepared_data/clean directory\n",
    "\n",
    "import glob\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# get all md files\n",
    "md_files = glob.glob(\"**/*.md\", recursive=True)\n",
    "\n",
    "# clean up the text using BeautifulSoup\n",
    "def clean_text(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning up text...\")\n",
    "for md_file in md_files:\n",
    "    # get the text from the file\n",
    "    with open(md_file, \"r\") as f:\n",
    "        text = f.read()\n",
    "    # clean up the text\n",
    "    text = clean_text(text)\n",
    "    print(text)\n",
    "    # save the cleaned up text to a file in the prepared_data/clean directory\n",
    "    clean_file = md_file.replace(\".md\", \".txt\")\n",
    "    clean_file = os.path.join(\"prepared_data/clean\", clean_file)\n",
    "    #create the file if it does not exist\n",
    "    if not os.path.exists(os.path.dirname(clean_file)):\n",
    "        os.makedirs(os.path.dirname(clean_file))\n",
    "    # save the cleaned up text to the file\n",
    "    with open(clean_file, \"w\") as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir prepared_data/summary_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all files in the clean directory\n",
    "clean_files = glob.glob(\"prepared_data/clean/**/*.txt\", recursive=True)\n",
    "text = \"\"\n",
    "with open(clean_files[15],\"r\") as f:\n",
    "    text = f.read()\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "prompt_summary = \"\"\"\n",
    "Objective: Summarize an internal resource document tailored for developers at Bitly, ensuring brevity and retention of all critical information.\n",
    "\n",
    "Task:\n",
    "Generate a comprehensive yet succinct summary of the following document text, ensuring all vital information is retained for effective utilization in vector DB and LLM operations.\n",
    "\n",
    "Document Text: \n",
    "{text}\n",
    "\n",
    "Summary:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"temperature\": 0,  # Temperature controls the degree of randomness in token selection.\n",
    "    \"max_output_tokens\": 2000,  # Token limit determines the maximum amount of text output.\n",
    "    # \"top_p\": 0.8,  # Tokens are selected from most probable to least until the sum of their probabilities equals the top_p value.\n",
    "    # \"top_k\": 40,  # A top_k of 1 means the selected token is the most probable among all tokens.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_file(clean_file):\n",
    "    # get the text from the file\n",
    "    with open(clean_file, \"r\") as f:\n",
    "        text = f.read()\n",
    "    # prepare prompt\n",
    "    formatted_prompt = prompt_summary.format(text=text)\n",
    "    summary = model.predict(formatted_prompt, **parameters)\n",
    "    # save the summary to a file in the prepared_data/summary_short directory\n",
    "    summary_file = clean_file.replace(\"clean\", \"summary_short\")\n",
    "    #create the file if it does not exist\n",
    "    if not os.path.exists(os.path.dirname(summary_file)):\n",
    "        os.makedirs(os.path.dirname(summary_file))\n",
    "    # save the summary to the file\n",
    "    with open(summary_file, \"w\") as f:\n",
    "        f.write(summary.text)\n",
    "    print(\"Completed summarizing file: \", clean_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(clean_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "# If you want to limit it to 10 workers (files) at a time:\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=20) as executor:\n",
    "    results = list(executor.map(summarize_file, clean_files[340:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the summary to each file in the clean directory\n",
    "# save the file to the prepared_data/clean_plus_summary directory\n",
    "\n",
    "def add_summary_to_file(clean_file):\n",
    "    # get the text from the file\n",
    "    with open(clean_file, \"r\") as f:\n",
    "        text = f.read()\n",
    "    # get the summary from the file\n",
    "    summary_file = clean_file.replace(\"clean\", \"summary_short\")\n",
    "    # check if the file exists, if not, print error and return\n",
    "    if not os.path.exists(summary_file):\n",
    "        print(\"Error: Summary file does not exist: \", summary_file)\n",
    "        return\n",
    "    with open(summary_file, \"r\") as f:\n",
    "        summary = f.read()\n",
    "    # add the summary to the text\n",
    "    text = \"---------- Summary: \\n\" + summary + \"\\n----------End Summary \\n\\n\" + text \n",
    "    # save the text to a file in the prepared_data/clean_plus_summary directory\n",
    "    clean_plus_summary_file = clean_file.replace(\"clean\", \"clean_plus_summary\")\n",
    "    #create the file if it does not exist\n",
    "    if not os.path.exists(os.path.dirname(clean_plus_summary_file)):\n",
    "        os.makedirs(os.path.dirname(clean_plus_summary_file))\n",
    "    # save the text to the file\n",
    "    with open(clean_plus_summary_file, \"w\") as f:\n",
    "        f.write(text)\n",
    "    print(\"Completed adding summary to file: \", clean_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file in the clean directory, add the summary to the file and save it to the clean_plus_summary directory\n",
    "\n",
    "print(\"Adding summary to files...\")\n",
    "for clean_file in clean_files[]:\n",
    "    add_summary_to_file(clean_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a new blob container in the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "\n",
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload all files in the prepared_data/clean_plus_summary directory to the blob container\n",
    "\n",
    "import glob\n",
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "\n",
    "print(bucket)\n",
    "\n",
    "# Get all files in the clean_plus_summary directory\n",
    "clean_plus_summary_files = glob.glob(\"prepared_data/clean_plus_summary/**/*.txt\", recursive=True)\n",
    "\n",
    "# Upload all files in the clean_plus_summary directory to the blob container\n",
    "for clean_plus_summary_file in clean_plus_summary_files:\n",
    "    # remove 'prepared_data/clean_plus_summary/' from file name\n",
    "    file_name = clean_plus_summary_file.replace(\"prepared_data/clean_plus_summary/\", \"\")\n",
    "    \n",
    "    # create a blob directory name and append the filename\n",
    "    blob_file_path = \"prepared_data_clean_plus_summary/\" + file_name\n",
    "    \n",
    "    print(\"Uploading file: \", clean_plus_summary_file, \" to blob: \", blob_file_path)\n",
    "\n",
    "    # create a new blob object and upload the file\n",
    "    blob = bucket.blob(blob_file_path)  # create a blob object with the right name\n",
    "    response = blob.upload_from_filename(clean_plus_summary_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://console.cloud.google.com/storage/browser/bitly-enterprise-search-docs;tab=objects?forceOnBucketsSortingFiltering=true&project=bitly-ai-experiments&prefix=&forceOnObjectsSortingFiltering=false\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import concurrent.futures\n",
    "import time\n",
    "import json\n",
    "\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "model = TextGenerationModel.from_pretrained('text-bison@latest')\n",
    "\n",
    "# Assume model and parameters are defined elsewhere in your code.\n",
    "# model = YourModelHere()\n",
    "parameters = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_output_tokens\": 500,\n",
    "}\n",
    "\n",
    "# Sample prompts\n",
    "prompt = \"\"\"\n",
    "Objective: Generate a fitting title and description for the following document text. Description should be either 1 or 2 sentences long.\n",
    "Response format should be JSON in this format: {{\"title\": \"title\", \"description\": \"description\"}}\n",
    "Document Text: \n",
    "{text}\n",
    "\n",
    "Response JSON: {{\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_and_save_title_description(clean_file):\n",
    "    with open(clean_file, \"r\") as f:\n",
    "        text = f.read()\n",
    "    \n",
    "    # Title extraction\n",
    "    formatted_prompt = prompt.format(text=text)\n",
    "    response = model.predict(formatted_prompt, **parameters).text\n",
    "     \n",
    "    # add { to the beginning of the response\n",
    "    response = \"{\" + response\n",
    "\n",
    "    try:\n",
    "        # parse the response as json\n",
    "        response = json.loads(response)\n",
    "\n",
    "        # Extract title and description from response\n",
    "        title = response[\"title\"]\n",
    "        description = response[\"description\"]\n",
    "    except:\n",
    "        # if it fails, print error and return\n",
    "        print(\"Error: Failed to parse response as JSON: \", response)\n",
    "        return\n",
    "    \n",
    "    # Save title\n",
    "    title_file = clean_file.replace(\"clean_plus_summary\", \"title\")\n",
    "    title_dir = os.path.dirname(title_file)\n",
    "    if not os.path.exists(title_dir):\n",
    "        os.makedirs(title_dir)\n",
    "    with open(title_file, \"w\") as f:\n",
    "        f.write(title)\n",
    "    \n",
    "    # Save description\n",
    "    description_file = clean_file.replace(\"clean_plus_summary\", \"description\")\n",
    "    description_dir = os.path.dirname(description_file)\n",
    "    if not os.path.exists(description_dir):\n",
    "        os.makedirs(description_dir)\n",
    "    with open(description_file, \"w\") as f:\n",
    "        f.write(description)\n",
    "\n",
    "    print(f\"Completed extracting title and description for: {clean_file}\")\n",
    "\n",
    "    time.sleep(10)  # Pauses execution for 5 seconds to avoid rate limiting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Replace this with your actual list of files.\n",
    "clean_plus_summary_files = glob.glob(\"prepared_data/clean_plus_summary/**/*.txt\", recursive=True)\n",
    "\n",
    "extract_and_save_title_description(clean_plus_summary_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this with your actual list of files.\n",
    "clean_plus_summary_files = glob.glob(\"prepared_data/clean_plus_summary/**/*.txt\", recursive=True)\n",
    "\n",
    "# Using concurrent futures to parallelize the process.\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    results = list(executor.map(extract_and_save_title_description, clean_plus_summary_files[260:300]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure each clean_with_summary file has a title and description file\n",
    "\n",
    "import glob\n",
    "\n",
    "# Get all files in the clean_plus_summary directory\n",
    "clean_plus_summary_files = glob.glob(\"prepared_data/clean_plus_summary/**/*.txt\", recursive=True)\n",
    "\n",
    "outstanding_files =[]\n",
    "\n",
    "# check if each clean_plus_summary file has a title and description file\n",
    "for clean_plus_summary_file in clean_plus_summary_files:\n",
    "    # Replace 'clean_plus_summary' in the filename with 'title' and 'description' to get the title and description filenames.\n",
    "    title_file = clean_plus_summary_file.replace(\"clean_plus_summary\", \"title\")\n",
    "    description_file = clean_plus_summary_file.replace(\"clean_plus_summary\", \"description\")\n",
    "    \n",
    "    # check if the title and description files exist and save it in an array to be used by ThreadPoolExecutor later\n",
    "    if not os.path.exists(title_file):\n",
    "        outstanding_files.append(clean_plus_summary_file)\n",
    "\n",
    "print (len(outstanding_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using concurrent futures to parallelize the process.\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    results = list(executor.map(extract_and_save_title_description, outstanding_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "def generate_metadata(file_name):\n",
    "    \"\"\"\n",
    "    Generate metadata for a given file name by reading previously generated title\n",
    "    and description from the filesystem.\n",
    "    \n",
    "    Parameters:\n",
    "        file_name (str): The name of the file for which metadata is generated.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing metadata for the file.\n",
    "    \"\"\"\n",
    "    # Replace 'clean_plus_summary' in the filename with 'title' and 'description' to get the title and description filenames.\n",
    "    title_file = file_name.replace(\"clean_plus_summary\", \"title\")\n",
    "    description_file = file_name.replace(\"clean_plus_summary\", \"description\")\n",
    "\n",
    "    # make sure title and description files exist, if not return set title and description to the file name\n",
    "\n",
    "    try:\n",
    "        # Read the title and description from the filesystem.\n",
    "        with open(title_file, \"r\") as f:\n",
    "            title = f.read().strip()\n",
    "        with open(description_file, \"r\") as f:\n",
    "            description = f.read().strip()\n",
    "    except:\n",
    "        return {\n",
    "            \"title\": file_name,\n",
    "            \"description\": file_name,\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"description\": description,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_gcs_uri(file_name, gcs_bucket_name):\n",
    "    \"\"\"\n",
    "    Construct the Google Cloud Storage URI for a given file name.\n",
    "    \n",
    "    Parameters:\n",
    "        file_name (str): The name of the file for which the URI is constructed.\n",
    "        gcs_bucket_name (str): The name of the Google Cloud Storage bucket.\n",
    "    \n",
    "    Returns:\n",
    "        str: The GCS URI for the file.\n",
    "    \"\"\"\n",
    "    # replace / with _ to create a valid GCS URI\n",
    "    gcs_uri = file_name.replace(\"/\", \"_\")\n",
    "\n",
    "    # add the bucket name to the GCS URI\n",
    "    gcs_uri = f\"gs://{gcs_bucket_name}/prepared_data_clean_plus_summary/{gcs_uri}\"\n",
    "\n",
    "    return gcs_uri\n",
    "\n",
    "def create_metadata_jsonl(clean_plus_summary_files, gcs_bucket_name, output_filename='metadata.jsonl'):\n",
    "    \"\"\"\n",
    "    Create a JSONL file containing metadata for each file in `clean_plus_summary_files`.\n",
    "    \n",
    "    Parameters:\n",
    "        clean_plus_summary_files (list of str): List of file paths to be processed.\n",
    "        gcs_bucket_name (str): The name of the Google Cloud Storage bucket.\n",
    "        output_filename (str): The name of the output JSONL file.\n",
    "    \"\"\"\n",
    "    with open(output_filename, 'w') as jsonl_file:\n",
    "        for i, clean_plus_summary_file in enumerate(clean_plus_summary_files):\n",
    "            # Remove 'prepared_data/clean_plus_summary/' from file name\n",
    "            file_name = clean_plus_summary_file.replace(\"prepared_data/clean_plus_summary/\", \"\")\n",
    "            \n",
    "            # Construct the URI for the GCS location\n",
    "            gcs_uri = get_gcs_uri(file_name, gcs_bucket_name)\n",
    "            \n",
    "            # Create metadata JSON object\n",
    "            metadata = {\n",
    "                \"id\": f\"doc-{i}\",\n",
    "                \"structData\": generate_metadata(clean_plus_summary_file),\n",
    "                \"content\": {\n",
    "                    \"mimeType\": \"text/plain\",  # Adjust MIME type if needed\n",
    "                    \"uri\": gcs_uri\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # Write JSON object to JSONL file as a new line\n",
    "            jsonl_file.write(json.dumps(metadata) + '\\n')\n",
    "\n",
    "# Usage example:\n",
    "\n",
    "# Get all files in the clean_plus_summary directory\n",
    "clean_plus_summary_files = glob.glob(\"prepared_data/clean_plus_summary/**/*.txt\", recursive=True)\n",
    "\n",
    "# Create the JSONL file\n",
    "create_metadata_jsonl(clean_plus_summary_files, BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload all files in the prepared_data/clean_plus_summary directory to the blob container\n",
    "# put all files in the same directory, replace file path / with _ to get the file name\n",
    "\n",
    "import glob\n",
    "\n",
    "# Get all files in the clean_plus_summary directory\n",
    "clean_plus_summary_files = glob.glob(\"prepared_data/clean_plus_summary/**/*.txt\", recursive=True)\n",
    "\n",
    "# Upload all files in the clean_plus_summary directory to the blob container\n",
    "for clean_plus_summary_file in clean_plus_summary_files:\n",
    "    # remove 'prepared_data/clean_plus_summary/' from file name\n",
    "    file_name = clean_plus_summary_file.replace(\"prepared_data/clean_plus_summary/\", \"\")\n",
    "    \n",
    "    # create a blob directory name and append the filename\n",
    "    blob_file_path = \"prepared_data_clean_plus_summary/\" + file_name\n",
    "    \n",
    "    print(\"Uploading file: \", clean_plus_summary_file, \" to blob: \", blob_file_path)\n",
    "\n",
    "    # create a new blob object and upload the file\n",
    "    blob = bucket.blob(blob_file_path)  # create a blob object with the right name\n",
    "    response = blob.upload_from_filename(clean_plus_summary_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload all files in the prepared_data/clean_plus_summary directory to the blob container\n",
    "\n",
    "import glob\n",
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "\n",
    "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
    "\n",
    "# Get all files in the clean_plus_summary directory\n",
    "clean_plus_summary_files = glob.glob(\"prepared_data/clean_plus_summary/**/*.txt\", recursive=True)\n",
    "\n",
    "# Upload all files in the clean_plus_summary directory to the blob container\n",
    "for clean_plus_summary_file in clean_plus_summary_files:\n",
    "    # remove 'prepared_data/clean_plus_summary/' from file name\n",
    "    file_name = clean_plus_summary_file.replace(\"prepared_data/clean_plus_summary/\", \"\")\n",
    "\n",
    "    #replace / to _ in filename\n",
    "    file_name = file_name.replace(\"/\", \"_\")\n",
    "    \n",
    "    # create a blob directory name and append the filename\n",
    "    blob_file_path = \"prepared_data_clean_plus_summary/\" + file_name\n",
    "    \n",
    "    print(\"Uploading file: \", clean_plus_summary_file, \" to blob: \", blob_file_path)\n",
    "\n",
    "    # create a new blob object and upload the file\n",
    "    blob = bucket.blob(blob_file_path)  # create a blob object with the right name\n",
    "    blob.upload_from_filename(clean_plus_summary_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file:  metadata.jsonl  to blob:  prepared_data_clean_plus_summary/metadata.jsonl\n"
     ]
    }
   ],
   "source": [
    "# upload metadata.jsonl to prepared_data_clean_plus_summary/\n",
    "\n",
    "# create a blob directory name and append the filename\n",
    "blob_file_path = \"prepared_data_clean_plus_summary/metadata.jsonl\"\n",
    "\n",
    "print(\"Uploading file: \", \"metadata.jsonl\", \" to blob: \", blob_file_path)\n",
    "\n",
    "# create a new blob object and upload the file\n",
    "blob = bucket.blob(blob_file_path)  # create a blob object with the right name\n",
    "blob.upload_from_filename(\"metadata.jsonl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
